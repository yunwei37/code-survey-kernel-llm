
\section{\sys: A Multi-Agent Framework for Socio-Technical Analysis}

To systematically analyze the decades of unstructured development history within the Linux kernel—a massive legacy codebase built long before the AI era—we designed \sys, a multi-agent framework that bridges the gap between pre-AI development artifacts and modern AI capabilities. Unlike single-model approaches and existing multi-agent systems~\cite{lmase2024,agent4se2024} that rely on generic roles, \sys's core principle is to create a high-fidelity simulation of the collaborative, role-based social structure of the open-source community itself. By instantiating \textbf{Empirically-Grounded Personas}—agents representing actual key developers—the framework captures the nuanced and often competing perspectives that shape the kernel's evolution, enabling AI systems to understand and work with legacy codebases containing decades of implicit knowledge.

This socio-technical approach draws inspiration from established social science techniques, particularly survey methodology used in sociological research. By treating LLM agents as human participants in a socio-technical system, \sys employs carefully designed surveys that agents answer based on development artifacts (commits, emails, patches). This transforms unstructured artifacts—capable of scaling to 15,000+ commits and 150,000+ emails—into structured, analyzable datasets by capturing not just \emph{what} changed, but the diverse perspectives of the specific individuals who drove those changes.

\subsection{Design Goals}

The design of \sys is guided by four core principles derived from observing the Linux development process. First, \textbf{Faithful Stakeholder Modeling} (G-1) requires that agents embody the distinct perspectives of \textbf{real kernel participants}, mirroring the real-world tension between goals like feature velocity, long-term stability (a maintainer's view), and risk mitigation (a security developer's view). Second, \textbf{Question-Driven, Evidence-Based Analytics} (G-2) mandates that the system answer high-level natural language questions with quantitative analysis and \textbf{explicit, traceable evidence}, reflecting the kernel community's norm where any claim must be backed by references to specific commits, mailing list threads, or performance data. Third, \textbf{Scalable and Economical Operation} (G-3) ensures the framework can process millions of artifacts cost-effectively through a \textbf{tiered inference pipeline}, where cheap, heuristic-based agents handle the majority of simple tasks, reserving powerful models for the complex reasoning performed by the core developer agents—mimicking the kernel's use of automated bots to triage issues before they reach human maintainers. Finally, \textbf{Measurable Reliability and Consensus} (G-4) requires the system to quantify its own internal agreement and reliability by measuring inter-agent agreement, building a trust model analogous to the kernel's \texttt{Reviewed-by} and \texttt{Tested-by} tags, ensuring that outputs are the result of a \textbf{consensus between key developer perspectives}.


\subsection{Simulating the Kernel Development Workflow}

The \sys architecture answers a user's query by simulating the lifecycle of a change within the Linux kernel community. The process is broken into phases that mirror how a patch moves from submission to integration.

\emph{Figure X: The architecture of \sys, which simulates the kernel development workflow to analyze historical artifacts.}

\begin{enumerate}
\item \textbf{Phase 1: Query Decomposition \& Artifact Retrieval.} The process begins when the \textbf{Orchestrator} agent parses a user's natural language query. It identifies the relevant historical artifacts—a set of commits, patch series, and mailing list threads—that serve as the subject of the analysis. This is analogous to a developer focusing on a specific problem or proposed change.

\item \textbf{Phase 2: Automated Triage (CI Pipeline Simulation).} The retrieved artifacts are first passed to a \textbf{Triage Crew} of lightweight, heuristic-based agents. This simulates the kernel's automated CI systems and bots. \texttt{SyzbotTwin} flags artifacts related to fuzzer-found bugs, while \texttt{CheckpatchBot} annotates for style or common errors. This phase enriches the artifacts with objective metadata before they undergo expert human review.

\item \textbf{Phase 3: Asynchronous Community Review (Mailing List Simulation).} This is the core analytical phase, simulating the peer review process on a mailing list. A dynamically assembled \textbf{Persona Crew}, consisting of Empirically-Grounded Personas for relevant developers (e.g., Alexei Starovoitov, Brendan Gregg), analyzes the artifacts in parallel. Each agent assesses the change from its unique perspective, conditioned by its personal knowledge base, and records its findings in a structured \textbf{survey}. This survey-based approach, borrowed from social science methodology, enables quantitative socio-technical analysis by transforming qualitative developer perspectives into analyzable data.

\item \textbf{Phase 4: Consensus Building \& Maintainer Decision (Resolution Simulation).} This phase simulates the process of achieving community consensus and receiving a maintainer's final judgment. The \textbf{Critic-Resolver} agent aggregates the surveys from the Persona Crew and calculates an agreement score (e.g., Cohen's Kappa, $\kappa$). If disagreement is high, a "debate" is triggered where agents must argue their positions by citing evidence, mimicking a mailing list discussion. The final, resolved view represents the community consensus, analogous to a maintainer applying a \texttt{Reviewed-by} tag or merging the patch.

\item \textbf{Phase 5: Insight Synthesis \& Reporting.} In the final phase, the \textbf{Data-Synthesizer} agent takes the structured, consensus-driven data from the simulation. It performs any required quantitative analysis (e.g., running \texttt{DuckDB} queries), generates charts and tables, and composes a final, human-readable report that answers the user's original query.
\end{enumerate}


\subsection{Empirically-Grounded Personas}

Central to \sys's fidelity is the deployment of Empirically-Grounded Personas—computational agents that embody the expertise and perspectives of actual kernel contributors. Each persona's analytical framework is systematically constructed from the developer's historical contributions, enabling the replication of their domain-specific insights and decision-making patterns. This approach operationalizes the tacit knowledge embedded within the community by transforming individual developer histories into structured analytical lenses. Each persona is underpinned by a comprehensive Knowledge Base, implemented as a dedicated vector store for Retrieval-Augmented Generation (RAG), which encompasses: (i) all commits authored by the developer, (ii) their complete mailing list correspondence, (iii) patches formally reviewed (identified through \texttt{Reviewed-by:} tags), and (iv) documented positions from design discussions and technical presentations.

This architecture enables highly specialized, perspective-driven analysis that mirrors the heterogeneous expertise within the kernel community. For instance, the \textbf{Alexei Starovoitov agent}, representing the eBPF maintainer, evaluates changes through the lens of verifier safety and API stability, leveraging its repository of thousands of accepted and rejected BPF patches. Conversely, the \textbf{Brendan Gregg agent}, embodying observability expertise, assesses the same changes based on their utility for performance tracing tools, drawing upon extensive knowledge of \texttt{perf} and \texttt{bpftrace} evolution. The \textbf{KP Singh agent} applies a security-focused perspective, analyzing code patterns against a corpus of Linux Security Module (LSM) discussions and BPF hardening initiatives. Through this multi-perspective approach, \sys enables systematic socio-technical analysis by leveraging the diverse viewpoints encoded in developer histories.


\subsection{Illustrative Workflow in Action}

To demonstrate the framework's capability, consider how \sys would answer the query: \textbf{"How did the refactoring of verifier loops after kernel v5.10 affect BPF program size limits?"}

The framework follows a story-like progression that simulates the community's problem-solving process.

\begin{enumerate}
\item \textbf{Survey Design \& Scoping}: The \textbf{Orchestrator} agent first parses the query. It identifies the core topics (verifier, performance, limits) and the relevant timeframe (post-v5.10). Based on this, it selects a tailored survey schema designed for analyzing architectural changes. This survey includes questions about motivation, technical risk, performance impact, and security implications, capturing multiple stakeholder perspectives. It then retrieves the relevant commits and mailing list discussions for analysis.

\item \textbf{Multi-Agent Analysis}: The artifacts are passed to the \textbf{Persona Crew}. For this query, a set of agents related to the query are activated. Each agent independently analyzes the artifacts and completes the survey from its unique perspective. For example, the \textbf{Alexei Starovoitov agent}, drawing from its maintainer memory, completes its survey noting the motivation was "refactoring for performance" with a "medium" technical risk. The \textbf{Daniel Borkmann agent} concurs on the motivation but assesses the risk as "low," citing its knowledge of the extensive test suite that was developed concurrently. Meanwhile, the \textbf{Brendan Gregg agent}, focusing on observability, highlights in its survey that the primary "impact" was enabling more powerful tracing tools.

\item \textbf{Consensus Building}: The \textbf{Critic-Resolver} agent gathers the completed surveys. It calculates the inter-agent agreement (Cohen's kappa, $\kappa$) and flags a disagreement on the "risk" assessment ("medium" vs. "low"). This triggers a simulated debate where the agents exchange their core evidence. The Alexei agent cites a past verifier bug, while the Daniel agent provides a link to the specific test-suite emails. They reconcile to a final consensus assessment of "low-to-medium risk, mitigated by extensive testing," which is recorded in the final survey data.

\item \textbf{Data Integration \& Synthesis}: Finally, the \textbf{Data-Synthesizer} agent takes the final, structured survey data. It performs a quantitative query on its dataset to plot the \texttt{BPF\_MAX\_INSNS} constant over different kernel versions, generating a chart that visually confirms the impact on program size limits. This transforms the multi-perspective analysis into a queryable insight, effectively integrating 30+ years of unstructured knowledge. The agent then composes the final answer, weaving together the narrative (the "why" from the surveys) and the quantitative proof (the chart) into a single, evidence-backed report.
\end{enumerate}
